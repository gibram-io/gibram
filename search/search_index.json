{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GibRAM","text":"<p>Graph in-Buffer Retrieval &amp; Associative Memory \u2022 v0.2.0</p> <p>GibRAM is an in-memory knowledge graph server designed for retrieval augmented generation (RAG) workflows. It combines graph storage with vector search so related information stays connected in memory.</p>"},{"location":"#what-is-gibram","title":"What is GibRAM?","text":"<ul> <li>In-Memory &amp; Ephemeral: Data lives in RAM with configurable time-to-live. Built for short-lived analysis, not persistent storage.</li> <li>Graph + Vectors Together: Stores entities, relationships, and document chunks alongside their embeddings in a unified structure.</li> <li>Graph-Aware Retrieval: Supports both semantic search and graph traversal, retrieving context that pure vector search might miss.</li> <li>Python SDK: GraphRAG-style workflow for indexing documents and querying with minimal code.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Choose your path:</p> <ul> <li>Run Server - Install and run GibRAM server (port 6161)</li> <li>Use Python SDK - Index documents and query in 10 lines of Python</li> </ul>"},{"location":"#why-gibram","title":"Why GibRAM?","text":"<p>Problem: Vector search alone often misses important context. If a query mentions \"Einstein\", traditional RAG might retrieve chunks about Einstein, but miss related entities like \"Theory of Relativity\" or \"Nobel Prize\" that aren't semantically similar to the query.</p> <p>Solution: GibRAM stores knowledge as a graph. When you query for \"Einstein\", it retrieves: 1. Semantically similar chunks (via embeddings) 2. Connected entities and relationships (via graph traversal) 3. Community summaries (via hierarchical clustering)</p> <p>This gives you richer, more complete context for generation.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    A[Documents] --&gt; B[Chunk]\n    B --&gt; C[Extract Entities&lt;br/&gt;&amp; Relationships]\n    C --&gt; D[Embed]\n    D --&gt; E[Store in Graph]\n    E --&gt; F[Query]\n\n    style A fill:#e3f2fd\n    style C fill:#ff6b6b\n    style D fill:#4ecdc4\n    style F fill:#95e1d3</code></pre> <ol> <li>Server runs on port 6161, manages sessions (isolated data per project)</li> <li>SDK handles chunking, extraction (via LLM), embedding, and storage</li> <li>Query combines vector similarity + graph traversal for complete results</li> </ol>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph Clients\n        CLI[CLI Client]\n        SDK[Python SDK]\n        Custom[Custom Go Client]\n    end\n\n    subgraph Server[\"Server Layer\"]\n        TCP[TCP Server]\n        Proto[Protobuf Codec]\n        Auth[RBAC Auth]\n        Rate[Rate Limiter]\n    end\n\n    subgraph Engine[\"Query Engine\"]\n        Eng[Engine]\n        QLog[Query Log LRU]\n    end\n\n    subgraph Storage[\"Session Storage\"]\n        SM[Session Manager]\n        SS1[Session Store 1]\n        SS2[Session Store 2]\n        SSN[Session Store N]\n    end\n\n    subgraph Indices[\"Per-Session Indices\"]\n        Doc[Documents]\n        TU[TextUnits]\n        Ent[Entities]\n        Rel[Relationships]\n        Com[Communities]\n        VecTU[HNSW Index&lt;br/&gt;TextUnits]\n        VecEnt[HNSW Index&lt;br/&gt;Entities]\n        VecCom[HNSW Index&lt;br/&gt;Communities]\n    end\n\n    subgraph Persistence[\"Persistence Layer\"]\n        WAL[Write-Ahead Log]\n        Snap[Snapshots]\n        Rec[Recovery]\n    end\n\n    CLI --&gt; TCP\n    SDK --&gt; TCP\n    Custom --&gt; TCP\n    TCP --&gt; Proto\n    Proto --&gt; Auth\n    Auth --&gt; Rate\n    Rate --&gt; Eng\n    Eng --&gt; SM\n    SM --&gt; SS1\n    SM --&gt; SS2\n    SM --&gt; SSN\n    SS1 --&gt; Doc\n    SS1 --&gt; TU\n    SS1 --&gt; Ent\n    SS1 --&gt; Rel\n    SS1 --&gt; Com\n    TU --&gt; VecTU\n    Ent --&gt; VecEnt\n    Com --&gt; VecCom\n    Eng --&gt; WAL\n    WAL --&gt; Snap\n    Snap --&gt; Rec</code></pre> <p>Session-based multi-tenancy: Each session is an isolated namespace with automatic TTL cleanup (absolute + idle timeout). Sessions are ephemeral by design. When TTL expires or server restarts, data is gone (unless persistence is enabled).</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>HNSW Vector Index: Fast approximate nearest neighbor search (O(log N))</li> <li>Hierarchical Leiden Clustering: Automatic community detection at multiple levels</li> <li>Protobuf Protocol: Efficient binary wire format for production use</li> <li>Custom Components: Swap chunkers, extractors, or embedders in Python SDK</li> <li>Optional Persistence: WAL + Snapshot for durability (disabled by default)</li> </ul>"},{"location":"#system-requirements","title":"System Requirements","text":"<ul> <li>Server: Go 1.24+, 2GB+ RAM recommended</li> <li>Python SDK: Python 3.8+</li> <li>LLM API: OpenAI API key for extraction and embeddings</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ol> <li>Start the server - Get GibRAM running locally</li> <li>Index your first documents - Try the Python SDK</li> <li>Configure for production - Security, TLS, auth</li> </ol>"},{"location":"#support","title":"Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Documentation: This site</li> <li>License: MIT</li> </ul>"},{"location":"getting-started/python-sdk/","title":"Use Python SDK (v0.2.0)","text":"<p>Get from zero to working GraphRAG in 5 minutes. The SDK handles chunking, extraction, embedding, and storage automatically.</p>"},{"location":"getting-started/python-sdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ installed</li> <li>GibRAM Server running on port 6161 (see how)</li> <li>OpenAI API Key for extraction and embeddings</li> </ul>"},{"location":"getting-started/python-sdk/#install","title":"Install","text":"<pre><code>pip install gibram\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"getting-started/python-sdk/#hero-example","title":"Hero Example","text":"<p>Save this as <code>demo.py</code>:</p> <pre><code>from gibram import GibRAMIndexer\n\n# Initialize indexer (connects to localhost:6161)\nindexer = GibRAMIndexer(session_id=\"my-project\")\n\n# Index documents (automatic: chunk \u2192 extract \u2192 embed \u2192 store)\nstats = indexer.index_documents([\n    \"Albert Einstein was born in 1879 in Ulm, Germany.\",\n    \"He developed the theory of relativity in 1905.\",\n    \"Einstein received the Nobel Prize in Physics in 1921.\",\n])\n\nprint(f\"\u2713 Indexed {stats.entities_extracted} entities\")\nprint(f\"\u2713 Created {stats.relationships_extracted} relationships\")\n\n# Query: vector search + graph traversal\nresult = indexer.query(\"Einstein's achievements\", top_k=5)\n\nfor entity in result.entities:\n    print(f\"{entity.title} ({entity.type}): {entity.score:.3f}\")\n</code></pre> <p>Run it:</p> <pre><code>python demo.py\n</code></pre> <p>Expected Output:</p> <pre><code>Processing documents: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:15&lt;00:00,  5.2s/doc]\n\u2713 Indexed 8 entities\n\u2713 Created 5 relationships\n\nALBERT EINSTEIN (person): 0.912\nTHEORY OF RELATIVITY (concept): 0.856\nNOBEL PRIZE IN PHYSICS (achievement): 0.798\n</code></pre>"},{"location":"getting-started/python-sdk/#what-just-happened","title":"What Just Happened?","text":"<p>The SDK executed this pipeline automatically:</p> <pre><code>graph LR\n    A[Documents] --&gt; B[1. Chunk&lt;br/&gt;512 tokens]\n    B --&gt; C[2. Extract&lt;br/&gt;GPT-4o]\n    C --&gt; D[3. Embed&lt;br/&gt;OpenAI API]\n    D --&gt; E[4. Store&lt;br/&gt;GibRAM]\n    E --&gt; F[5. Query&lt;br/&gt;HNSW + Graph]\n\n    C -.-&gt;|Entities +&lt;br/&gt;Relationships| D\n    B -.-&gt;|Text Units| D\n\n    style C fill:#ff6b6b\n    style D fill:#4ecdc4\n    style F fill:#95e1d3</code></pre> <p>Pipeline Steps:</p> <ol> <li>Chunked documents into ~512 token chunks (with 50 token overlap)</li> <li>Extracted entities and relationships using GPT-4o (\ud83d\udcb0 LLM calls here)</li> <li>Embedded chunks and entities via OpenAI API (text-embedding-3-small, 1536 dims)</li> <li>Stored everything in GibRAM server under session \"my-project\"</li> <li>Queried using HNSW vector search + graph traversal</li> </ol> <p>Cost: ~$0.10 for 3 documents (3 LLM calls + 12 embeddings)</p>"},{"location":"getting-started/python-sdk/#important-dimension-mismatch","title":"\u26a0\ufe0f Important: Dimension Mismatch","text":"<p>Server and SDK embedding dimensions MUST match.</p> <ul> <li>Server default: 1536 (configure via <code>--dim</code> flag or config file)</li> <li>SDK default: 1536 (OpenAI text-embedding-3-small)</li> </ul> <p>If using different embedding model:</p> <pre><code># Server: gibram-server --dim 768\n# SDK:\nindexer = GibRAMIndexer(\n    embedding_dimensions=768  # MUST match server\n)\n</code></pre> <p>Symptom if mismatch: Runtime error <code>dimension mismatch: expected 1536, got 768</code></p>"},{"location":"getting-started/python-sdk/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/python-sdk/#step-by-step-tutorial","title":"Step-by-Step Tutorial","text":"<p>Python SDK Quickstart - 6-step walkthrough with indexing, querying, and RAG examples</p>"},{"location":"getting-started/python-sdk/#deep-dives","title":"Deep Dives","text":"<ul> <li>SDK Overview - Architecture, components, and customization</li> <li>Server Configuration - TLS, auth, persistence, TTL</li> <li>Troubleshooting - Connection refused, dimension mismatch, OpenAI errors</li> </ul>"},{"location":"getting-started/server/","title":"Run GibRAM Server (v0.2.0)","text":"<p>Get GibRAM server running on your machine. This guide covers the fastest path to a working server.</p>"},{"location":"getting-started/server/#prerequisites","title":"Prerequisites","text":"<ul> <li>Operating System: Linux, macOS, or Windows (WSL)</li> <li>Memory: 1GB+ RAM recommended</li> <li>Port: 6161 must be available (default)</li> </ul>"},{"location":"getting-started/server/#install-methods","title":"Install Methods","text":""},{"location":"getting-started/server/#option-1-binary-install-fastest","title":"Option 1: Binary Install (Fastest)","text":"<p>Download and run the pre-built binary:</p> <pre><code># Install via script\ncurl -fsSL https://gibram.io/install.sh | sh\n\n# Verify installation\ngibram-server --version\n</code></pre> <p>The binary will be installed to <code>/usr/local/bin/gibram-server</code>.</p>"},{"location":"getting-started/server/#option-2-docker-recommended-for-production","title":"Option 2: Docker (Recommended for Production)","text":"<pre><code># Run server\ndocker run -d \\\n  -p 6161:6161 \\\n  --name gibram \\\n  gibramio/gibram:latest\n</code></pre> <p>For persistent data:</p> <pre><code>docker run -d \\\n  -p 6161:6161 \\\n  -v gibram-data:/var/lib/gibram/data \\\n  --name gibram \\\n  gibramio/gibram:latest\n</code></pre>"},{"location":"getting-started/server/#option-3-build-from-source","title":"Option 3: Build from Source","text":"<p>Requires Go 1.24+:</p> <pre><code>git clone https://github.com/gibram-io/gibram.git\ncd gibram\ngo build -o gibram-server ./cmd/server\n./gibram-server --insecure\n</code></pre>"},{"location":"getting-started/server/#start-the-server","title":"Start the Server","text":""},{"location":"getting-started/server/#development-mode-insecure","title":"Development Mode (Insecure)","text":"<p>For local development, use insecure mode (no TLS, no auth):</p> <pre><code>gibram-server --insecure\n</code></pre> <p>Expected output:</p> <pre><code>INFO  GibRAM v0.2.0 starting...\nINFO    Address:    :6161\nINFO    Data dir:   ./data\nINFO    Vector dim: 1536\nINFO    Log level:  info\nINFO    Protocol:   GibRAM Protocol v1 (proto3)\nWARN  Running in INSECURE mode (no TLS, no auth)\nINFO  GibRAM Protobuf Server listening on :6161\nINFO    Authentication: disabled (insecure)\nINFO    Max frame size: 67108864 bytes\nINFO    Rate limit: 1000 req/s (burst: 100)\nINFO    Metrics:    enabled\nINFO    Memory:     monitoring enabled (max 1024MB)\n</code></pre> <p>\u26a0\ufe0f CRITICAL: <code>--insecure</code> mode disables all security. NEVER use in production.</p>"},{"location":"getting-started/server/#verify-server-is-running","title":"Verify Server is Running","text":"<p>Test with CLI client:</p> <pre><code># In another terminal\ngibram-cli -h localhost:6161 -insecure true\n\n# Inside CLI, type:\ngibram&gt; PING\n# Expected: PONG (XXXms)\n\ngibram&gt; INFO\n# Expected: Server stats with 0 entities/documents\n</code></pre>"},{"location":"getting-started/server/#configuration","title":"Configuration","text":""},{"location":"getting-started/server/#server-listens-on","title":"Server Listens On","text":"<ul> <li>Address: <code>0.0.0.0:6161</code> (all interfaces, port 6161)</li> <li>Protocol: GibRAM Protocol v1 (Protobuf over TCP)</li> <li>Data Directory: <code>./data</code> (default)</li> <li>Vector Dimension: 1536 (default, matches OpenAI text-embedding-3-small)</li> </ul>"},{"location":"getting-started/server/#common-flags","title":"Common Flags","text":"<pre><code>gibram-server \\\n  --addr :6161 \\                    # Bind address\n  --data ./data \\                   # Data directory\n  --dim 1536 \\                      # Vector dimension\n  --log-level info \\                # Log level (debug|info|warn|error)\n  --session-cleanup-interval 60s    # Session TTL check interval\n</code></pre>"},{"location":"getting-started/server/#config-file-advanced","title":"Config File (Advanced)","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>server:\n  addr: \":6161\"\n  data_dir: \"./data\"\n  vector_dim: 1536\n\nlogging:\n  level: \"info\"\n  format: \"text\"\n</code></pre> <p>Run with config:</p> <pre><code>gibram-server --config config.yaml\n</code></pre> <p>Precedence: CLI flags override config file values.</p>"},{"location":"getting-started/server/#production-setup","title":"Production Setup","text":"<p>For production deployment:</p> <ol> <li>Use Config File: See Configuration Basics</li> <li>Enable TLS: Required for network security</li> <li>Enable Auth: API key authentication</li> <li>Resource Limits: Docker memory limits recommended</li> </ol> <p>DO NOT use <code>--insecure</code> flag in production.</p>"},{"location":"getting-started/server/#session-concept","title":"Session Concept","text":"<p>GibRAM is session-based. Each session is an isolated namespace:</p> <ul> <li>Session ID: Required in all API calls (e.g., \"my-project\")</li> <li>TTL: Sessions auto-expire (default: no limit, but configurable)</li> <li>Isolation: Data in session \"A\" cannot be accessed by session \"B\"</li> </ul> <p>Sessions are created automatically on first write. No explicit setup needed.</p>"},{"location":"getting-started/server/#data-lifecycle","title":"Data Lifecycle","text":"<p>By Default (In-Memory Ephemeral): - Data lives only in RAM - Server restart = all data lost - Session TTL expires = data cleaned up</p> <p>Optional (Persistence Enabled): - WAL (Write-Ahead Log) for durability - Snapshot for fast restore - See Configuration Basics</p>"},{"location":"getting-started/server/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/server/#server-wont-start","title":"Server Won't Start","text":"<p>Symptom: Error \"address already in use\"</p> <p>Cause: Port 6161 is taken</p> <p>Fix: <pre><code># Check what's using port 6161\nlsof -i :6161\n\n# Use different port\ngibram-server --addr :6162 --insecure\n</code></pre></p>"},{"location":"getting-started/server/#client-cant-connect","title":"Client Can't Connect","text":"<p>Symptom: \"connection refused\"</p> <p>Check: 1. Server is running: <code>ps aux | grep gibram-server</code> 2. Port is correct: Server logs show \"listening on :6161\" 3. Firewall allows port 6161</p>"},{"location":"getting-started/server/#out-of-memory","title":"Out of Memory","text":"<p>Symptom: Server killed or slows down</p> <p>Cause: Too much data in sessions without cleanup</p> <p>Fix: 1. Set session TTL (see Configuration) 2. Manually delete sessions via protocol 3. Increase Docker memory limit 4. Monitor with metrics (see Observability)</p>"},{"location":"getting-started/server/#next-steps","title":"Next Steps","text":"<ul> <li>Use Python SDK - Index your first documents</li> <li>Configuration Basics - TLS, auth, persistence</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"sdks/python/","title":"Python SDK (v0.2.0)","text":"<p>The GibRAM Python SDK provides a GraphRAG-style workflow for indexing documents and querying knowledge graphs with minimal code.</p>"},{"location":"sdks/python/#what-is-the-python-sdk","title":"What is the Python SDK?","text":"<p>A high-level library that automates the GraphRAG pipeline:</p> <pre><code>Documents \u2192 Chunk \u2192 Extract \u2192 Embed \u2192 Store \u2192 Query\n</code></pre> <p>Instead of manually handling each step, you write:</p> <pre><code>from gibram import GibRAMIndexer\n\nindexer = GibRAMIndexer(session_id=\"my-project\")\nstats = indexer.index_documents([\"Your document text here\"])\nresult = indexer.query(\"Your question\")\n</code></pre> <p>The SDK handles: \u2705 Text chunking (with overlap) \u2022 \u2705 Entity &amp; relationship extraction (via LLM) \u2022 \u2705 Embedding generation (via OpenAI) \u2022 \u2705 Graph storage (via GibRAM protocol) \u2022 \u2705 Community detection (hierarchical clustering) \u2022 \u2705 Hybrid query (vector + graph traversal)</p>"},{"location":"sdks/python/#when-to-use-the-sdk","title":"When to Use the SDK","text":"<p>Use the Python SDK when: - You want GraphRAG workflow out-of-the-box - You're building RAG applications - You need automatic entity extraction - You want customizable components (chunker, extractor, embedder)</p> <p>Don't use the SDK when: - You need custom protocol operations (use low-level client) - You're building non-Python applications (use Go client or implement protocol) - You have pre-extracted entities (use protocol directly)</p>"},{"location":"sdks/python/#architecture","title":"Architecture","text":""},{"location":"sdks/python/#high-level-vs-low-level","title":"High-Level vs Low-Level","text":"<p>High-Level (<code>GibRAMIndexer</code>): <pre><code># All-in-one workflow\nindexer = GibRAMIndexer(session_id=\"my-project\")\nstats = indexer.index_documents(documents)\n</code></pre></p> <p>Low-Level (<code>_Client</code>): <pre><code># Manual protocol operations\nfrom gibram._client import _Client\n\nclient = _Client(host=\"localhost\", port=6161, session_id=\"test\")\nclient.connect()\n\ndoc_id = client.add_document(external_id=\"doc-001\", filename=\"file.txt\")\nentity_id = client.add_entity(\n    external_id=\"ent-001\",\n    title=\"EINSTEIN\",\n    entity_type=\"person\",\n    description=\"...\",\n    embedding=[0.1, 0.2, ...]  # You provide embedding\n)\n</code></pre></p> <p>When to use low-level: Custom workflows, pre-computed embeddings, fine-grained control.</p>"},{"location":"sdks/python/#component-architecture","title":"Component Architecture","text":"<p>The SDK is modular. You can swap components:</p> <pre><code>from gibram import GibRAMIndexer\nfrom gibram.chunkers import TokenChunker\nfrom gibram.extractors import OpenAIExtractor\nfrom gibram.embedders import OpenAIEmbedder\n\n# Default (OpenAI for everything)\nindexer = GibRAMIndexer(session_id=\"my-project\")\n\n# Custom components\nindexer = GibRAMIndexer(\n    session_id=\"my-project\",\n    chunker=TokenChunker(chunk_size=1024, chunk_overlap=100),\n    extractor=OpenAIExtractor(model=\"gpt-4o-mini\"),\n    embedder=OpenAIEmbedder(model=\"text-embedding-3-large\", dimensions=3072)\n)\n</code></pre> <p>Interfaces: - <code>BaseChunker</code> - Splits text into chunks - <code>BaseExtractor</code> - Extracts entities and relationships - <code>BaseEmbedder</code> - Generates embeddings</p> <p>See Custom Components for implementation guide.</p>"},{"location":"sdks/python/#installation","title":"Installation","text":"<pre><code>pip install gibram\n</code></pre> <p>Requirements: - Python 3.8+ - GibRAM server running (see Getting Started)</p> <p>Dependencies (auto-installed): - <code>protobuf</code> - Protocol communication - <code>openai</code> - Default LLM and embeddings - <code>tqdm</code> - Progress bars</p>"},{"location":"sdks/python/#quick-example","title":"Quick Example","text":"<pre><code>from gibram import GibRAMIndexer\n\n# Initialize\nindexer = GibRAMIndexer(\n    session_id=\"my-project\",\n    llm_api_key=\"sk-...\",  # Or set OPENAI_API_KEY\n)\n\n# Index\nstats = indexer.index_documents([\n    \"Albert Einstein developed the theory of relativity.\",\n    \"He received the Nobel Prize in Physics in 1921.\",\n])\n\nprint(f\"Entities extracted: {stats.entities_extracted}\")\nprint(f\"Relationships: {stats.relationships_extracted}\")\n\n# Query\nresult = indexer.query(\"Einstein's achievements\", top_k=5)\n\nfor entity in result.entities:\n    print(f\"{entity.title}: {entity.score:.3f}\")\n</code></pre>"},{"location":"sdks/python/#key-concepts","title":"Key Concepts","text":""},{"location":"sdks/python/#session-isolation","title":"Session Isolation","text":"<p>Each <code>GibRAMIndexer</code> instance operates in an isolated session:</p> <pre><code># Project A data\nindexer_a = GibRAMIndexer(session_id=\"project-a\")\nindexer_a.index_documents(docs_a)\n\n# Project B data (completely separate)\nindexer_b = GibRAMIndexer(session_id=\"project-b\")\nindexer_b.index_documents(docs_b)\n\n# Queries only see data from their session\nresult_a = indexer_a.query(\"query\")  # Only searches project-a data\n</code></pre> <p>Best Practice: Use one session per project or experiment.</p>"},{"location":"sdks/python/#indexing-pipeline","title":"Indexing Pipeline","text":"<p>When you call <code>index_documents()</code>:</p> <ol> <li>Chunking: Splits documents into ~512 token chunks (configurable)</li> <li>Extraction: Calls LLM to extract entities and relationships per chunk</li> <li>Deduplication: Merges entities with same title across chunks</li> <li>Embedding: Generates embeddings for chunks and entities</li> <li>Storage: Stores in GibRAM server via protocol</li> <li>Linking: Links entities to their source chunks</li> <li>Clustering: Detects communities (if enabled)</li> </ol> <p>Cost: Each chunk = 1 LLM call + embeddings. See Quickstart for estimation.</p>"},{"location":"sdks/python/#query-execution","title":"Query Execution","text":"<p>When you call <code>query()</code>:</p> <ol> <li>Embedding: Generates query embedding</li> <li>Vector Search: Finds similar chunks, entities, communities</li> <li>Graph Traversal: Expands via relationships (k-hops)</li> <li>Ranking: Combines similarity scores</li> <li>Return: Sorted results with scores</li> </ol> <p>Results include: - <code>entities</code> - Relevant entities (with descriptions) - <code>text_units</code> - Source chunks (with content) - <code>communities</code> - Cluster summaries (if available)</p>"},{"location":"sdks/python/#configuration","title":"Configuration","text":""},{"location":"sdks/python/#essential-parameters","title":"Essential Parameters","text":"<pre><code>indexer = GibRAMIndexer(\n    session_id=\"required-unique-id\",  # REQUIRED\n    host=\"localhost\",                 # Server host\n    port=6161,                        # Server port\n)\n</code></pre>"},{"location":"sdks/python/#llm-configuration","title":"LLM Configuration","text":"<pre><code>indexer = GibRAMIndexer(\n    llm_provider=\"openai\",       # Only OpenAI currently\n    llm_api_key=\"sk-...\",        # Or env OPENAI_API_KEY\n    llm_model=\"gpt-4o\",          # Default: gpt-4o\n)\n</code></pre> <p>Supported models: - <code>gpt-4o</code> - Best quality (default) - <code>gpt-4o-mini</code> - Faster, cheaper - <code>gpt-4-turbo</code> - Alternative</p>"},{"location":"sdks/python/#embedding-configuration","title":"Embedding Configuration","text":"<pre><code>indexer = GibRAMIndexer(\n    embedding_provider=\"openai\",\n    embedding_model=\"text-embedding-3-small\",  # Default\n    embedding_dimensions=1536,                 # MUST match server\n)\n</code></pre> <p>\u26a0\ufe0f CRITICAL: <code>embedding_dimensions</code> must match server <code>vector_dim</code>.</p> <p>Supported models: - <code>text-embedding-3-small</code> - 1536 dims (default) - <code>text-embedding-3-large</code> - 3072 dims (higher quality) - <code>text-embedding-ada-002</code> - 1536 dims (legacy)</p>"},{"location":"sdks/python/#chunking-configuration","title":"Chunking Configuration","text":"<pre><code>indexer = GibRAMIndexer(\n    chunk_size=512,      # Tokens per chunk\n    chunk_overlap=50,    # Overlap between chunks\n)\n</code></pre> <p>Trade-offs: - Larger chunks: Fewer LLM calls (cheaper), but less precise retrieval - Smaller chunks: More LLM calls (costlier), but more precise retrieval</p>"},{"location":"sdks/python/#community-detection","title":"Community Detection","text":"<pre><code>indexer = GibRAMIndexer(\n    auto_detect_communities=True,  # Default: True\n    community_resolution=1.0,      # Higher = more granular clusters\n)\n</code></pre>"},{"location":"sdks/python/#error-handling","title":"Error Handling","text":"<pre><code>from gibram.exceptions import (\n    GibRAMError,\n    ConfigurationError,\n    ConnectionError,\n    ExtractionError,\n    EmbeddingError,\n)\n\ntry:\n    indexer = GibRAMIndexer(session_id=\"test\")\n    stats = indexer.index_documents(documents)\nexcept ConfigurationError as e:\n    print(f\"Configuration issue: {e}\")\nexcept ConnectionError as e:\n    print(f\"Can't reach server: {e}\")\nexcept ExtractionError as e:\n    print(f\"LLM extraction failed: {e}\")\nexcept EmbeddingError as e:\n    print(f\"Embedding generation failed: {e}\")\n</code></pre>"},{"location":"sdks/python/#performance","title":"Performance","text":"<p>Typical throughput: - Indexing: ~5-10 documents/minute (depends on LLM API latency) - Querying: ~100-200ms per query</p> <p>Bottlenecks: 1. LLM API calls (dominant) 2. Embedding API calls 3. Network latency to GibRAM server</p> <p>Optimization tips: - Increase <code>batch_size</code> for faster embedding calls - Use <code>gpt-4o-mini</code> instead of <code>gpt-4o</code> for faster extraction - Disable <code>auto_detect_communities</code> if not needed</p>"},{"location":"sdks/python/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Step-by-step examples</li> <li>Indexing Workflow - Deep dive into indexing</li> <li>Query Workflow - Deep dive into querying</li> <li>Custom Components - Build your own chunker/extractor/embedder</li> </ul>"},{"location":"sdks/python/quickstart/","title":"Python SDK Quickstart (v0.2.0)","text":"<p>Get started with GibRAM Python SDK in 5 minutes. This guide walks through installation, indexing, and querying.</p>"},{"location":"sdks/python/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ installed</li> <li>GibRAM server running on port 6161 (see Run Server)</li> <li>OpenAI API key for extraction and embeddings</li> </ul>"},{"location":"sdks/python/quickstart/#step-1-install-sdk","title":"Step 1: Install SDK","text":"<pre><code>pip install gibram\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import gibram; print(gibram.__version__)\"\n# Expected: 0.2.0\n</code></pre>"},{"location":"sdks/python/quickstart/#step-2-set-api-key","title":"Step 2: Set API Key","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre> <p>Or pass directly to <code>GibRAMIndexer</code> (see Step 3).</p>"},{"location":"sdks/python/quickstart/#step-3-index-documents","title":"Step 3: Index Documents","text":"<p>Create <code>index.py</code>:</p> <pre><code>from gibram import GibRAMIndexer\n\n# Initialize indexer\nindexer = GibRAMIndexer(\n    session_id=\"quickstart-demo\",\n    host=\"localhost\",\n    port=6161,\n    # llm_api_key=\"sk-...\",  # Optional: override env var\n)\n\n# Sample documents about Einstein\ndocuments = [\n    \"Albert Einstein was born on March 14, 1879, in Ulm, Germany.\",\n    \"He developed the theory of relativity, one of the two pillars of modern physics.\",\n    \"Einstein received the Nobel Prize in Physics in 1921 for his discovery of the photoelectric effect.\",\n    \"He emigrated to the United States in 1933 and became an American citizen in 1940.\",\n]\n\n# Index documents\nprint(\"Indexing documents...\")\nstats = indexer.index_documents(\n    documents,\n    batch_size=10,\n    show_progress=True\n)\n\n# Print statistics\nprint(\"\\n=== Indexing Complete ===\")\nprint(f\"Documents indexed: {stats.documents_indexed}\")\nprint(f\"Text units created: {stats.text_units_created}\")\nprint(f\"Entities extracted: {stats.entities_extracted}\")\nprint(f\"Relationships extracted: {stats.relationships_extracted}\")\nprint(f\"Communities detected: {stats.communities_detected}\")\nprint(f\"Time taken: {stats.indexing_time_seconds:.2f}s\")\n</code></pre> <p>Run:</p> <pre><code>python index.py\n</code></pre> <p>Expected Output:</p> <pre><code>Indexing documents...\nProcessing documents: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:08&lt;00:00,  2.1s/doc]\nGenerating embeddings...\nStoring entities...\nStoring relationships...\n\n=== Indexing Complete ===\nDocuments indexed: 4\nText units created: 4\nEntities extracted: 12\nRelationships extracted: 8\nCommunities detected: 2\nTime taken: 8.45s\n</code></pre>"},{"location":"sdks/python/quickstart/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>Chunking: Each document was treated as one chunk (small docs)</li> <li>Extraction: GPT-4o extracted entities (Einstein, Germany, Nobel Prize, etc.) and relationships (born_in, received, etc.)</li> <li>Embedding: Generated embeddings for chunks and entities via OpenAI API</li> <li>Storage: Stored in GibRAM server under session \"quickstart-demo\"</li> <li>Clustering: Detected 2 communities (Einstein-related entities grouped)</li> </ol> <p>Cost: ~$0.10 for 4 documents (4 LLM calls + ~20 embeddings)</p>"},{"location":"sdks/python/quickstart/#step-4-query","title":"Step 4: Query","text":"<p>Create <code>query.py</code>:</p> <pre><code>from gibram import GibRAMIndexer\n\n# Connect to same session\nindexer = GibRAMIndexer(\n    session_id=\"quickstart-demo\",\n    host=\"localhost\",\n    port=6161,\n)\n\n# Query 1: Biographical information\nprint(\"Query: Where was Einstein born?\\n\")\nresult = indexer.query(\"Where was Einstein born?\", top_k=3)\n\nprint(f\"Found {len(result.entities)} entities:\")\nfor entity in result.entities[:3]:\n    print(f\"  - {entity.title} ({entity.type}): {entity.score:.3f}\")\n    print(f\"    {entity.description[:100]}...\")\n\nprint(f\"\\nFound {len(result.text_units)} text chunks:\")\nfor chunk in result.text_units[:2]:\n    print(f\"  - {chunk.content[:80]}... [score: {chunk.score:.3f}]\")\n\n# Query 2: Scientific achievements\nprint(\"\\n\" + \"=\"*50)\nprint(\"\\nQuery: Einstein's scientific contributions\\n\")\nresult = indexer.query(\"Einstein's scientific contributions\", top_k=5)\n\nprint(f\"Top Entities:\")\nfor entity in result.entities[:5]:\n    print(f\"  {entity.title} ({entity.type}): {entity.score:.3f}\")\n</code></pre> <p>Run:</p> <pre><code>python query.py\n</code></pre> <p>Expected Output:</p> <pre><code>Query: Where was Einstein born?\n\nFound 3 entities:\n  - ALBERT EINSTEIN (person): 0.923\n    German-born theoretical physicist who developed the theory of relativity...\n  - ULM (location): 0.887\n    City in Germany where Albert Einstein was born...\n  - GERMANY (location): 0.856\n    Country in Europe where Einstein was born...\n\nFound 2 text chunks:\n  - Albert Einstein was born on March 14, 1879, in Ulm, Germany. [score: 0.912]\n  - He emigrated to the United States in 1933 and became an American citizen... [score: 0.734]\n\n==================================================\n\nQuery: Einstein's scientific contributions\n\nTop Entities:\n  THEORY OF RELATIVITY (concept): 0.945\n  ALBERT EINSTEIN (person): 0.901\n  NOBEL PRIZE IN PHYSICS (achievement): 0.876\n  PHOTOELECTRIC EFFECT (concept): 0.834\n  MODERN PHYSICS (field): 0.789\n</code></pre>"},{"location":"sdks/python/quickstart/#understanding-results","title":"Understanding Results","text":"<pre><code>graph TD\n    Q[Query Text] --&gt; E[Generate Embedding]\n    E --&gt; VS[Vector Search]\n\n    VS --&gt; T[TextUnit Index&lt;br/&gt;top_k]\n    VS --&gt; EN[Entity Index&lt;br/&gt;top_k]\n    VS --&gt; C[Community Index&lt;br/&gt;top_k]\n\n    EN --&gt; GR[Graph Traversal&lt;br/&gt;k-hop expansion]\n\n    T --&gt; R[Ranked Results]\n    GR --&gt; R\n    C --&gt; R\n\n    R --&gt; U[User receives:&lt;br/&gt;entities + text_units + communities]\n\n    style VS fill:#4ecdc4\n    style GR fill:#ff6b6b\n    style R fill:#95e1d3</code></pre> <p>Entities: - Extracted concepts, people, places - Scored by semantic similarity to query - Includes descriptions (useful for LLM context)</p> <p>Text Units: - Original document chunks - Scored by embedding similarity - Raw text for citation</p> <p>Score Interpretation: - <code>0.9 - 1.0</code>: Highly relevant - <code>0.7 - 0.9</code>: Relevant - <code>&lt; 0.7</code>: May not be relevant (depends on threshold)</p>"},{"location":"sdks/python/quickstart/#step-5-build-rag-answer","title":"Step 5: Build RAG Answer","text":"<p>Create <code>rag.py</code>:</p> <pre><code>from gibram import GibRAMIndexer\nimport openai\n\nindexer = GibRAMIndexer(session_id=\"quickstart-demo\")\n\n# Query\nquestion = \"What are Einstein's major achievements?\"\nresult = indexer.query(question, top_k=5)\n\n# Build context from results\ncontext_parts = []\n\n# Add entity information\ncontext_parts.append(\"Relevant Entities:\")\nfor entity in result.entities[:3]:\n    context_parts.append(f\"- {entity.title}: {entity.description}\")\n\n# Add source chunks\ncontext_parts.append(\"\\nSource Text:\")\nfor chunk in result.text_units[:2]:\n    context_parts.append(f\"- {chunk.content}\")\n\ncontext = \"\\n\".join(context_parts)\n\n# Generate answer with GPT-4\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant. Answer based on the provided context.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n        }\n    ]\n)\n\nprint(\"Question:\", question)\nprint(\"\\nAnswer:\", response.choices[0].message.content)\nprint(\"\\nSources:\")\nfor i, chunk in enumerate(result.text_units[:2], 1):\n    print(f\"{i}. {chunk.content[:100]}...\")\n</code></pre> <p>Expected Output:</p> <pre><code>Question: What are Einstein's major achievements?\n\nAnswer: Albert Einstein's major achievements include developing the theory of \nrelativity, which became one of the two pillars of modern physics. He also \nreceived the Nobel Prize in Physics in 1921 for his discovery of the photoelectric \neffect, which was fundamental to the development of quantum theory.\n\nSources:\n1. He developed the theory of relativity, one of the two pillars of modern physics...\n2. Einstein received the Nobel Prize in Physics in 1921 for his discovery of the...\n</code></pre>"},{"location":"sdks/python/quickstart/#step-6-clean-up-optional","title":"Step 6: Clean Up (Optional)","text":"<p>Sessions persist until: 1. Server restarts (ephemeral mode) 2. TTL expires (if configured) 3. Manual deletion</p> <p>Note: Currently, session deletion requires low-level client or CLI. High-level SDK support coming in future versions.</p>"},{"location":"sdks/python/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"sdks/python/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>SDK Overview - Architecture and components</li> <li>Indexing Workflow - Deep dive into indexing pipeline</li> <li>Query Workflow - Deep dive into query execution</li> </ul>"},{"location":"sdks/python/quickstart/#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Custom Components - Build custom chunker/extractor/embedder</li> <li>Configuration - All parameters explained</li> <li>Examples - More code samples</li> </ul>"},{"location":"sdks/python/quickstart/#production-deployment","title":"Production Deployment","text":"<ul> <li>Server Configuration - TLS, auth, persistence</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"sdks/python/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"sdks/python/quickstart/#connection-refused","title":"\"Connection refused\"","text":"<p>Cause: Server not running.</p> <p>Fix: <pre><code># Start server\ngibram-server --insecure\n\n# Verify with CLI client\ngibram-cli -h localhost:6161 -insecure true\n# Then type: PING\n# Expected: PONG (XXXms)\n</code></pre></p>"},{"location":"sdks/python/quickstart/#dimension-mismatch","title":"\"Dimension mismatch\"","text":"<p>Cause: Server vector_dim \u2260 SDK embedding_dimensions.</p> <p>Fix: <pre><code># Check server dimension (via CLI: INFO command)\n# Then match in SDK:\nindexer = GibRAMIndexer(\n    embedding_dimensions=1536  # Match server\n)\n</code></pre></p>"},{"location":"sdks/python/quickstart/#openai-rate-limit","title":"\"OpenAI rate limit\"","text":"<p>Cause: Too many API calls too quickly.</p> <p>Fix: <pre><code># Reduce batch size\nstats = indexer.index_documents(\n    documents,\n    batch_size=5  # Slower but less likely to hit limits\n)\n</code></pre></p>"},{"location":"sdks/python/quickstart/#extraction-failed-for-chunk","title":"\"Extraction failed for chunk\"","text":"<p>Cause: LLM returned invalid JSON or timed out.</p> <p>Impact: That chunk is skipped (not a critical error).</p> <p>Fix: Retry indexing. If persists, check OpenAI service status.</p>"},{"location":"sdks/python/quickstart/#cost-estimation","title":"Cost Estimation","text":"<p>OpenAI Pricing (as of Jan 2024): - GPT-4o: ~$0.01 per chunk extraction - Embeddings (text-embedding-3-small): ~$0.0001 per embedding</p> <p>Example: - 100 documents - Average 4 chunks per document = 400 chunks - Cost: (400 \u00d7 $0.01) + (400 \u00d7 $0.0001) = $4.04</p> <p>Reduce costs: - Use <code>gpt-4o-mini</code> (cheaper) - Increase <code>chunk_size</code> (fewer chunks) - Disable <code>auto_detect_communities</code></p>"},{"location":"sdks/python/quickstart/#full-example-code","title":"Full Example Code","text":"<p>See complete runnable examples: - examples/basic_indexing.py - examples/custom_implementation.py</p>"},{"location":"server/configuration-basics/","title":"Configuration Basics (v0.2.0)","text":"<p>Configure GibRAM server for production: TLS, authentication, persistence, and resource limits.</p>"},{"location":"server/configuration-basics/#configuration-methods","title":"Configuration Methods","text":""},{"location":"server/configuration-basics/#1-config-file-recommended","title":"1. Config File (Recommended)","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>server:\n  addr: \":6161\"\n  data_dir: \"/var/lib/gibram/data\"\n  vector_dim: 1536\n\nlogging:\n  level: \"info\"\n  format: \"json\"\n  output: \"stdout\"\n</code></pre> <p>Run with config:</p> <pre><code>gibram-server --config config.yaml\n</code></pre>"},{"location":"server/configuration-basics/#2-cli-flags","title":"2. CLI Flags","text":"<p>Override config or use without config file:</p> <pre><code>gibram-server \\\n  --addr :6161 \\\n  --data /var/lib/gibram/data \\\n  --dim 1536 \\\n  --log-level info\n</code></pre>"},{"location":"server/configuration-basics/#3-precedence","title":"3. Precedence","text":"<p>CLI flags &gt; Config file &gt; Defaults</p>"},{"location":"server/configuration-basics/#core-settings","title":"Core Settings","text":""},{"location":"server/configuration-basics/#server","title":"Server","text":"<pre><code>server:\n  addr: \":6161\"              # Bind address (default: :6161)\n  data_dir: \"./data\"         # Data directory (default: ./data)\n  vector_dim: 1536           # Vector dimension (default: 1536)\n</code></pre> <p>\u26a0\ufe0f CRITICAL: <code>vector_dim</code> must match SDK embedding dimensions.</p> <p>Common Values: - <code>1536</code> - OpenAI text-embedding-3-small (default) - <code>768</code> - Sentence transformers, some open models - <code>3072</code> - OpenAI text-embedding-3-large</p> <p>Once set, cannot be changed without data loss (re-indexing required).</p>"},{"location":"server/configuration-basics/#logging","title":"Logging","text":"<pre><code>logging:\n  level: \"info\"              # debug | info | warn | error\n  format: \"text\"             # text | json\n  output: \"stdout\"           # stdout | file\n  file: \"\"                   # Log file path (if output=file)\n</code></pre> <p>Production Recommendation: Use <code>format: json</code> for structured logging (better for log aggregators).</p>"},{"location":"server/configuration-basics/#security","title":"Security","text":""},{"location":"server/configuration-basics/#tls-production-required","title":"TLS (Production Required)","text":"<p>Development (Auto-Cert):</p> <pre><code>tls:\n  auto_cert: true            # Generate self-signed cert\n</code></pre> <p>Production (Custom Certificates):</p> <pre><code>tls:\n  cert_file: \"/etc/gibram/certs/server.crt\"\n  key_file: \"/etc/gibram/certs/server.key\"\n  auto_cert: false           # Disable auto-cert\n</code></pre> <p>Generate Certificates:</p> <pre><code># Self-signed (testing)\nopenssl req -x509 -newkey rsa:4096 -nodes \\\n  -keyout server.key \\\n  -out server.crt \\\n  -days 365 \\\n  -subj \"/CN=localhost\"\n\n# Let's Encrypt (production)\ncertbot certonly --standalone -d yourdomain.com\n</code></pre> <p>\u26a0\ufe0f IMPORTANT:  - Without TLS, traffic is unencrypted - Client must skip verification for self-signed certs - Production should use CA-signed certificates</p>"},{"location":"server/configuration-basics/#authentication","title":"Authentication","text":"<p>API Key Authentication:</p> <pre><code>auth:\n  keys:\n    - id: \"admin\"\n      key: \"your-secure-admin-key-here\"\n      permissions: [\"admin\"]\n\n    - id: \"app-service\"\n      key: \"your-secure-app-key-here\"\n      permissions: [\"write\"]\n\n    - id: \"query-service\"\n      key: \"your-secure-query-key-here\"\n      permissions: [\"read\"]\n</code></pre> <p>Permission Levels: - <code>admin</code> - Full access (backup, sessions, all operations) - <code>write</code> - Read + write data (entities, relationships, queries) - <code>read</code> - Read-only (queries, get operations)</p> <p>Using API Key (Python SDK):</p> <pre><code>from gibram import GibRAMIndexer\n\nindexer = GibRAMIndexer(\n    session_id=\"my-project\",\n    api_key=\"your-secure-app-key-here\"  # Not yet supported in Python SDK\n)\n</code></pre> <p>Using API Key (Go Client):</p> <pre><code>config := client.DefaultPoolConfig()\nconfig.APIKey = \"your-secure-app-key-here\"\n\nc, err := client.NewClientWithConfig(\"localhost:6161\", \"session-id\", config)\n</code></pre> <p>\u26a0\ufe0f SECURITY NOTE: Store keys in environment variables or secrets manager, not in config files committed to git.</p>"},{"location":"server/configuration-basics/#rate-limiting","title":"Rate Limiting","text":"<pre><code>security:\n  max_frame_size: 67108864   # 64MB frame size limit\n  rate_limit: 1000           # Requests per second\n  rate_burst: 100            # Burst allowance\n  idle_timeout: 300s         # Idle connection timeout\n  unauth_timeout: 10s        # Timeout for unauthenticated connections\n  max_conns_per_ip: 50       # Max connections per IP\n</code></pre> <p>Adjust for Load: - High traffic: Increase <code>rate_limit</code> and <code>max_conns_per_ip</code> - Low resources: Decrease to prevent DoS - Long operations: Increase <code>idle_timeout</code></p>"},{"location":"server/configuration-basics/#persistence-optional","title":"Persistence (Optional)","text":"<p>By Default: GibRAM is ephemeral (in-memory only). Data lost on restart.</p> <p>Enable Persistence:</p> <p>Currently, WAL and snapshot are initialized but manual operation only. Commands available:</p> <ul> <li><code>SAVE</code> - Create snapshot (blocking)</li> <li><code>BGSAVE</code> - Create snapshot (background)</li> <li><code>LASTSAVE</code> - Get last save timestamp</li> </ul> <p>Future: Automatic persistence configuration.</p>"},{"location":"server/configuration-basics/#session-management","title":"Session Management","text":"<p>Session Cleanup Interval:</p> <pre><code>gibram-server --session-cleanup-interval 60s\n</code></pre> <p>Default: 60 seconds (check for expired sessions every minute)</p> <p>Session TTL (set via protocol or SDK):</p> <p>Currently configured per-session via protocol commands. SDK support coming in future versions.</p>"},{"location":"server/configuration-basics/#resource-limits","title":"Resource Limits","text":""},{"location":"server/configuration-basics/#memory","title":"Memory","text":"<p>Docker Memory Limit:</p> <pre><code># docker-compose.yml\nservices:\n  gibram:\n    deploy:\n      resources:\n        limits:\n          memory: 2G\n        reservations:\n          memory: 512M\n</code></pre> <p>Monitoring: Server tracks memory and logs warnings at 80%, 99%, 100%.</p>"},{"location":"server/configuration-basics/#vector-dimension-impact","title":"Vector Dimension Impact","text":"<p>Higher dimensions = more memory per vector: - 1536 dims: ~6KB per vector (float32) - 3072 dims: ~12KB per vector</p> <p>Estimate: 1M entities at 1536 dims \u2248 6GB RAM</p>"},{"location":"server/configuration-basics/#example-configurations","title":"Example Configurations","text":""},{"location":"server/configuration-basics/#development","title":"Development","text":"<pre><code>server:\n  addr: \":6161\"\n  data_dir: \"./data\"\n  vector_dim: 1536\n\ntls:\n  auto_cert: true\n\nlogging:\n  level: \"debug\"\n  format: \"text\"\n</code></pre> <p>Run: <pre><code>gibram-server --insecure  # Disable TLS &amp; auth for dev\n</code></pre></p>"},{"location":"server/configuration-basics/#production","title":"Production","text":"<pre><code>server:\n  addr: \":6161\"\n  data_dir: \"/var/lib/gibram/data\"\n  vector_dim: 1536\n\ntls:\n  cert_file: \"/etc/gibram/certs/server.crt\"\n  key_file: \"/etc/gibram/certs/server.key\"\n\nauth:\n  keys:\n    - id: \"production-app\"\n      key: \"{{ .Env.GIBRAM_API_KEY }}\"\n      permissions: [\"write\"]\n\nsecurity:\n  rate_limit: 5000\n  max_conns_per_ip: 100\n\nlogging:\n  level: \"info\"\n  format: \"json\"\n  output: \"file\"\n  file: \"/var/log/gibram/gibram.log\"\n</code></pre>"},{"location":"server/configuration-basics/#docker-production","title":"Docker Production","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  gibram:\n    image: gibramio/gibram:latest\n    ports:\n      - \"6161:6161\"\n\n    volumes:\n      - ./config.yaml:/etc/gibram/config.yaml:ro\n      - ./certs:/etc/gibram/certs:ro\n      - gibram-data:/var/lib/gibram/data\n\n    environment:\n      - GIBRAM_API_KEY=${GIBRAM_API_KEY}\n\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n\nvolumes:\n  gibram-data:\n</code></pre>"},{"location":"server/configuration-basics/#validation","title":"Validation","text":"<p>Test Configuration:</p> <pre><code># Dry-run (validates config)\ngibram-server --config config.yaml --help\n\n# Check server starts\ngibram-server --config config.yaml\n\n# Verify logs\ntail -f /var/log/gibram/gibram.log\n</code></pre> <p>Check Settings:</p> <pre><code># Use CLI\ngibram-cli -h localhost:6161\n\ngibram&gt; INFO\n# Shows vector_dim, session count, etc.\n</code></pre>"},{"location":"server/configuration-basics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"server/configuration-basics/#server-wont-start","title":"Server Won't Start","text":"<p>Symptom: \"Failed to load config\"</p> <p>Check: 1. YAML syntax valid: <code>yamllint config.yaml</code> 2. File permissions: <code>ls -la config.yaml</code> 3. Paths exist: <code>data_dir</code>, cert files</p>"},{"location":"server/configuration-basics/#tls-handshake-failed","title":"TLS Handshake Failed","text":"<p>Symptom: Client \"tls: handshake failure\"</p> <p>Causes: - Self-signed cert without skip-verify - Cert expired - Cert hostname mismatch</p> <p>Fix (Development): <pre><code># Python SDK (no skip-verify option yet)\n# Use --insecure mode on server instead\n</code></pre></p>"},{"location":"server/configuration-basics/#authentication-failed","title":"Authentication Failed","text":"<p>Symptom: \"unauthorized\" error</p> <p>Causes: - Wrong API key - Key not in server config - Insufficient permissions</p> <p>Fix: Verify key matches config, check permission level</p>"},{"location":"server/configuration-basics/#dimension-mismatch","title":"Dimension Mismatch","text":"<p>Symptom: Runtime error when adding entities/chunks</p> <p>Cause: Server <code>vector_dim</code> \u2260 client embedding dimension</p> <p>Fix: Restart server with correct <code>--dim</code> value (requires re-indexing)</p>"},{"location":"server/configuration-basics/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting Guide - Detailed error solutions</li> <li>SDK Configuration - Client-side setup</li> </ul>"},{"location":"server/troubleshooting/","title":"Troubleshooting (v0.2.0)","text":"<p>Common issues and solutions when running GibRAM server.</p>"},{"location":"server/troubleshooting/#server-issues","title":"Server Issues","text":""},{"location":"server/troubleshooting/#server-wont-start","title":"Server Won't Start","text":""},{"location":"server/troubleshooting/#port-already-in-use","title":"Port Already in Use","text":"<p>Symptom: <pre><code>Error: listen tcp :6161: bind: address already in use\n</code></pre></p> <p>Cause: Another process is using port 6161.</p> <p>Diagnosis: <pre><code># Check what's using the port\nlsof -i :6161\n\n# Or on Linux\nnetstat -tlnp | grep 6161\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Stop the conflicting process: <pre><code># Kill process using port 6161\nkill -9 &lt;PID&gt;\n</code></pre></p> </li> <li> <p>Use different port: <pre><code>gibram-server --addr :6162\n</code></pre></p> </li> <li> <p>Update SDK to match: <pre><code>indexer = GibRAMIndexer(\n    session_id=\"my-project\",\n    port=6162  # Match server port\n)\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#invalid-configuration","title":"Invalid Configuration","text":"<p>Symptom: <pre><code>Error: Failed to load config: yaml: unmarshal errors\n</code></pre></p> <p>Cause: Syntax error in <code>config.yaml</code>.</p> <p>Solutions:</p> <ol> <li> <p>Validate YAML syntax: <pre><code># Install yamllint\npip install yamllint\n\n# Check syntax\nyamllint config.yaml\n</code></pre></p> </li> <li> <p>Check indentation (YAML is whitespace-sensitive): <pre><code># \u2717 Wrong (mixed tabs/spaces)\nserver:\n    addr: \":6161\"\n\n# \u2713 Correct (2 spaces)\nserver:\n  addr: \":6161\"\n</code></pre></p> </li> <li> <p>Use example config: <pre><code>cp config.example.yaml config.yaml\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#permission-denied","title":"Permission Denied","text":"<p>Symptom: <pre><code>Error: failed to create data directory: permission denied\n</code></pre></p> <p>Cause: No write permission to data directory.</p> <p>Solutions:</p> <ol> <li> <p>Check permissions: <pre><code>ls -la ./data\n</code></pre></p> </li> <li> <p>Fix permissions: <pre><code># Create directory with correct permissions\nmkdir -p ./data\nchmod 755 ./data\n\n# Or run as user with permissions\nsudo chown -R $USER:$USER ./data\n</code></pre></p> </li> <li> <p>Use different directory: <pre><code>gibram-server --data ~/gibram-data\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#tls-certificate-issues","title":"TLS Certificate Issues","text":"<p>Symptom: <pre><code>Error: failed to configure TLS: tls: failed to find any PEM data\n</code></pre></p> <p>Cause: Certificate file not found or invalid.</p> <p>Solutions:</p> <ol> <li> <p>Verify cert files exist: <pre><code>ls -la /etc/gibram/certs/\n</code></pre></p> </li> <li> <p>Check file format (must be PEM): <pre><code>openssl x509 -in server.crt -text -noout\n</code></pre></p> </li> <li> <p>Generate new cert: <pre><code>openssl req -x509 -newkey rsa:4096 -nodes \\\n  -keyout server.key \\\n  -out server.crt \\\n  -days 365 \\\n  -subj \"/CN=localhost\"\n</code></pre></p> </li> <li> <p>Use auto-cert for development: <pre><code>tls:\n  auto_cert: true\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#server-crashes-or-hangs","title":"Server Crashes or Hangs","text":""},{"location":"server/troubleshooting/#out-of-memory","title":"Out of Memory","text":"<p>Symptom: - Server killed by OS - Logs: \"CRITICAL: Memory usage 2048MB / 2048MB\" - Docker: Container restarted</p> <p>Cause: Too much data in sessions without cleanup.</p> <p>Diagnosis: <pre><code># Check memory usage\nfree -h\n\n# Check server logs\ntail -f /var/log/gibram/gibram.log\n\n# Check Docker stats\ndocker stats gibram\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Increase memory limit (Docker): <pre><code># docker-compose.yml\ndeploy:\n  resources:\n    limits:\n      memory: 4G  # Increase from 2G\n</code></pre></p> </li> <li> <p>Enable session TTL (via protocol): <pre><code># Set sessions to expire after 1 hour\n# (SDK support coming in future version)\n</code></pre></p> </li> <li> <p>Manual cleanup: <pre><code># Delete expired sessions via CLI\ngibram-cli -h localhost:6161\n\ngibram&gt; LIST_SESSIONS\n# Check session IDs\n\ngibram&gt; DELETE_SESSION &lt;session_id&gt;\n</code></pre></p> </li> <li> <p>Reduce data volume:</p> </li> <li>Index fewer documents</li> <li>Use larger chunk_size (fewer chunks)</li> <li>Delete unused sessions</li> </ol>"},{"location":"server/troubleshooting/#too-many-connections","title":"Too Many Connections","text":"<p>Symptom: <pre><code>Error: max sessions limit reached (10000)\n</code></pre></p> <p>Cause: DoS protection triggered.</p> <p>Solutions:</p> <ol> <li> <p>Check active sessions: <pre><code>gibram-cli&gt; LIST_SESSIONS\n</code></pre></p> </li> <li> <p>Clean up old sessions: <pre><code># Delete specific session\ngibram-cli&gt; DELETE_SESSION old-session-id\n\n# Or restart server (if ephemeral mode)\n</code></pre></p> </li> <li> <p>Increase limit (code change required): <pre><code>// In pkg/engine/engine.go\nconst MaxSessions = 50000  // Increase from 10000\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#client-connection-issues","title":"Client Connection Issues","text":""},{"location":"server/troubleshooting/#connection-refused","title":"Connection Refused","text":"<p>Symptom (Python): <pre><code>ConnectionRefusedError: [Errno 111] Connection refused\n</code></pre></p> <p>Symptom (Go): <pre><code>failed to connect: dial tcp 127.0.0.1:6161: connect: connection refused\n</code></pre></p> <p>Cause: Server not running or wrong host/port.</p> <p>Solutions:</p> <ol> <li> <p>Check server is running: <pre><code>ps aux | grep gibram-server\n\n# Expected output:\n# user  12345  0.0  0.1  gibram-server --insecure\n</code></pre></p> </li> <li> <p>Verify port: <pre><code># Server logs should show:\n# INFO  GibRAM Protobuf Server listening on :6161\n</code></pre></p> </li> <li> <p>Test with CLI: <pre><code>gibram-cli -h localhost:6161 -insecure true\n\ngibram&gt; PING\n# Should return: PONG\n</code></pre></p> </li> <li> <p>Check firewall: <pre><code># Allow port 6161\nsudo ufw allow 6161/tcp\n</code></pre></p> </li> <li> <p>Docker network: <pre><code># If server in Docker, use host.docker.internal\n# Or run SDK in same network\ndocker network create gibram-network\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#tls-handshake-failed","title":"TLS Handshake Failed","text":"<p>Symptom: <pre><code>tls: handshake failure: remote error: tls: bad certificate\n</code></pre></p> <p>Cause: Certificate validation failed.</p> <p>Solutions:</p> <ol> <li>Development: Use insecure mode: <pre><code>gibram-server --insecure\n</code></pre></li> </ol> <pre><code># Client: no TLS config needed\nindexer = GibRAMIndexer(session_id=\"test\")\n</code></pre> <ol> <li> <p>Skip cert verification (Go client): <pre><code>config := client.DefaultPoolConfig()\nconfig.TLSEnabled = true\nconfig.TLSSkipVerify = true  // Dev only!\n</code></pre></p> </li> <li> <p>Production: Use valid certificate:</p> </li> <li>CA-signed certificate</li> <li>Hostname matches cert CN</li> <li>Certificate not expired</li> </ol>"},{"location":"server/troubleshooting/#authentication-failed","title":"Authentication Failed","text":"<p>Symptom: <pre><code>Error: unauthorized\n</code></pre></p> <p>Cause: Missing or invalid API key.</p> <p>Solutions:</p> <ol> <li> <p>Verify server requires auth: <pre><code># Server logs should show:\n# INFO  Authentication: enabled\n</code></pre></p> </li> <li> <p>Check API key matches config: <pre><code># config.yaml\nauth:\n  keys:\n    - id: \"app\"\n      key: \"your-key-here\"\n      permissions: [\"write\"]\n</code></pre></p> </li> <li> <p>Use correct key in client (Go): <pre><code>config := client.DefaultPoolConfig()\nconfig.APIKey = \"your-key-here\"\n</code></pre></p> </li> <li> <p>Development: Disable auth: <pre><code>gibram-server --insecure  # Disables both TLS and auth\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#data-issues","title":"Data Issues","text":""},{"location":"server/troubleshooting/#dimension-mismatch","title":"Dimension Mismatch","text":"<p>Symptom: <pre><code>Error: dimension mismatch: expected 1536, got 768\n</code></pre></p> <p>Cause: Server <code>vector_dim</code> \u2260 embedding dimension.</p> <p>Where it happens: - When adding TextUnit with embedding - When adding Entity with embedding - When adding Community with embedding</p> <p>Solutions:</p> <ol> <li> <p>Check server dimension: <pre><code>gibram-cli&gt; INFO\n# Look for: VectorDim: 1536\n</code></pre></p> </li> <li> <p>Match SDK to server: <pre><code># If server uses 1536\nindexer = GibRAMIndexer(\n    embedding_dimensions=1536  # Default, matches server\n)\n</code></pre></p> </li> <li> <p>Change server dimension (requires restart + re-index): <pre><code>gibram-server --dim 768\n</code></pre></p> </li> <li> <p>Use compatible embedding model: <pre><code># Server: vector_dim = 1536\n# SDK: Use OpenAI text-embedding-3-small (1536 dims)\nindexer = GibRAMIndexer(\n    embedding_model=\"text-embedding-3-small\",\n    embedding_dimensions=1536\n)\n</code></pre></p> </li> </ol> <p>\u26a0\ufe0f WARNING: Changing <code>vector_dim</code> requires re-indexing all data.</p>"},{"location":"server/troubleshooting/#session-not-found","title":"Session Not Found","text":"<p>Symptom: <pre><code>Error: session not found\n</code></pre></p> <p>Cause: Session expired or never created.</p> <p>Solutions:</p> <ol> <li> <p>Check session exists: <pre><code>gibram-cli&gt; LIST_SESSIONS\n</code></pre></p> </li> <li> <p>Session TTL expired:</p> </li> <li>Session auto-deleted after TTL</li> <li> <p>Re-index data in new session</p> </li> <li> <p>Server restarted (ephemeral mode):</p> </li> <li>All sessions lost on restart</li> <li> <p>Re-index data</p> </li> <li> <p>Wrong session ID: <pre><code># Check session_id spelling\nindexer = GibRAMIndexer(session_id=\"my-project\")  # Must match exactly\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#duplicate-entity-error","title":"Duplicate Entity Error","text":"<p>Symptom: <pre><code>Error: entity with title \"EINSTEIN\" already exists\n</code></pre></p> <p>Cause: Entity with same title already in session.</p> <p>Behavior: SDK handles this automatically by: 1. Checking if entity exists by title 2. Reusing existing entity ID 3. Linking to text units</p> <p>If you see this error: - You're using low-level client directly - Check for existing entity before adding</p>"},{"location":"server/troubleshooting/#sdk-issues","title":"SDK Issues","text":""},{"location":"server/troubleshooting/#openai-api-errors","title":"OpenAI API Errors","text":""},{"location":"server/troubleshooting/#rate-limit","title":"Rate Limit","text":"<p>Symptom: <pre><code>openai.RateLimitError: Rate limit exceeded\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Reduce batch size: <pre><code>stats = indexer.index_documents(\n    documents,\n    batch_size=5  # Slower but less likely to hit rate limit\n)\n</code></pre></p> </li> <li> <p>Add delay between batches (custom implementation needed)</p> </li> <li> <p>Upgrade OpenAI plan for higher rate limits</p> </li> </ol>"},{"location":"server/troubleshooting/#invalid-api-key","title":"Invalid API Key","text":"<p>Symptom: <pre><code>openai.AuthenticationError: Incorrect API key provided\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check API key: <pre><code>echo $OPENAI_API_KEY\n</code></pre></p> </li> <li> <p>Verify key is valid on OpenAI dashboard</p> </li> <li> <p>Pass key explicitly: <pre><code>indexer = GibRAMIndexer(\n    llm_api_key=\"sk-...\",\n    session_id=\"test\"\n)\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#quota-exceeded","title":"Quota Exceeded","text":"<p>Symptom: <pre><code>openai.RateLimitError: You exceeded your current quota\n</code></pre></p> <p>Solutions:</p> <ol> <li>Check quota on OpenAI billing dashboard</li> <li>Add payment method or upgrade plan</li> </ol>"},{"location":"server/troubleshooting/#extraction-failed","title":"Extraction Failed","text":"<p>Symptom: <pre><code>Warning: Extraction failed for chunk: ...\n</code></pre></p> <p>Cause: LLM returned invalid JSON or timed out.</p> <p>Impact: Chunk is skipped (indexing continues for other chunks).</p> <p>Solutions:</p> <ol> <li>Check OpenAI service status</li> <li>Retry indexing (transient errors)</li> <li>Check chunk content (very long/malformed text can cause issues)</li> </ol>"},{"location":"server/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"server/troubleshooting/#slow-indexing","title":"Slow Indexing","text":"<p>Symptom: Indexing takes &gt; 10s per document.</p> <p>Causes:</p> <ol> <li>LLM API latency (main bottleneck)</li> <li>Large documents \u2192 many chunks</li> <li>Slow network to OpenAI API</li> </ol> <p>Solutions:</p> <ol> <li> <p>Use faster model: <pre><code>indexer = GibRAMIndexer(\n    llm_model=\"gpt-4o-mini\"  # Faster than gpt-4o\n)\n</code></pre></p> </li> <li> <p>Increase chunk size (fewer LLM calls): <pre><code>indexer = GibRAMIndexer(\n    chunk_size=1024  # Larger chunks = fewer API calls\n)\n</code></pre></p> </li> <li> <p>Disable community detection: <pre><code>indexer = GibRAMIndexer(\n    auto_detect_communities=False\n)\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#slow-queries","title":"Slow Queries","text":"<p>Symptom: Query takes &gt; 1s.</p> <p>Causes:</p> <ol> <li>Large result set (high top_k)</li> <li>Many entities in session</li> <li>Deep graph traversal</li> </ol> <p>Solutions:</p> <ol> <li> <p>Reduce top_k: <pre><code>result = indexer.query(\"query\", top_k=5)  # Instead of 50\n</code></pre></p> </li> <li> <p>Limit result types: <pre><code>result = indexer.query(\n    \"query\",\n    include_entities=True,\n    include_text_units=False,  # Skip if not needed\n    include_communities=False\n)\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#docker-issues","title":"Docker Issues","text":""},{"location":"server/troubleshooting/#container-exits-immediately","title":"Container Exits Immediately","text":"<p>Symptom: <pre><code>docker ps\n# gibram container not listed\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check logs: <pre><code>docker logs gibram\n</code></pre></p> </li> <li> <p>Run interactively to see error: <pre><code>docker run -it --rm \\\n  -p 6161:6161 \\\n  gibramio/gibram:latest\n</code></pre></p> </li> <li> <p>Common causes:</p> </li> <li>Invalid config mounted</li> <li>Port conflict (6161 already used on host)</li> <li>Memory limit too low</li> </ol>"},{"location":"server/troubleshooting/#cant-connect-from-host","title":"Can't Connect from Host","text":"<p>Symptom: Python SDK on host can't reach Docker container.</p> <p>Solutions:</p> <ol> <li> <p>Check port mapping: <pre><code>docker ps\n# Should show: 0.0.0.0:6161-&gt;6161/tcp\n</code></pre></p> </li> <li> <p>Use correct host: <pre><code># From host machine\nindexer = GibRAMIndexer(\n    host=\"localhost\",  # Or \"127.0.0.1\"\n    port=6161\n)\n</code></pre></p> </li> <li> <p>Check Docker network: <pre><code>docker inspect gibram\n# Look for \"Ports\" section\n</code></pre></p> </li> </ol>"},{"location":"server/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"server/troubleshooting/#gather-information","title":"Gather Information","text":"<p>When reporting issues, include:</p> <ol> <li> <p>GibRAM version: <pre><code>gibram-server --version\n</code></pre></p> </li> <li> <p>Server logs: <pre><code># Last 100 lines\ntail -n 100 /var/log/gibram/gibram.log\n\n# Or Docker logs\ndocker logs gibram --tail 100\n</code></pre></p> </li> <li> <p>Configuration (sanitized): <pre><code># config.yaml (remove sensitive keys)\n</code></pre></p> </li> <li> <p>Reproduction steps:</p> </li> <li>Minimal code example</li> <li>Expected vs actual behavior</li> </ol>"},{"location":"server/troubleshooting/#resources","title":"Resources","text":"<ul> <li>GitHub Issues: github.com/gibram-io/gibram/issues</li> <li>Documentation: This site</li> <li>Examples: <code>examples/</code> directory in repo</li> </ul>"},{"location":"server/troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Basics - Prevent common issues</li> <li>Python SDK Quickstart - SDK setup</li> </ul>"}]}